{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28d55a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\llmenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afc0954a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6688c0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'train_0',\n",
       " 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n",
       " 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n",
       " 'topic': 'get a check-up'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "002d1b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\n",
      "#Person2#: I found it would be a good idea to get a check-up.\n",
      "#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\n",
      "#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\n",
      "#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\n",
      "#Person2#: Ok.\n",
      "#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\n",
      "#Person2#: Yes.\n",
      "#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\n",
      "#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\n",
      "#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\n",
      "#Person2#: Ok, thanks doctor.\n"
     ]
    }
   ],
   "source": [
    "example = dataset['train'][0]\n",
    "dialogure_lines = example['dialogue'].split('\\n')\n",
    "\n",
    "for line in dialogure_lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43f6a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Model\n",
    "model_name = 'google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name , torch_dtype = torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "785e02d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8774,  296,    1]])\n",
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "#Example :\n",
    "\n",
    "prompt = \"Hello world\"\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\") #pt is for pytorch\n",
    "\n",
    "print(tokens[\"input_ids\"])\n",
    "print(tokenizer.decode(tokens[\"input_ids\"][0],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eb3ce5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856\n",
      " All model parameters:247577856 \n",
      " Percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# To check the trainable parameters\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\n All model parameters:{all_model_params} \\n Percentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40173379",
   "metadata": {},
   "source": [
    "# Test the Model with Zero Shot Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15a52e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "Summary of the Dialogure is: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n"
     ]
    }
   ],
   "source": [
    "# Lets utilize this dialogue for inference\n",
    "example = dataset['test'][200]\n",
    "dialogure_lines = example['dialogue'].split('\\n')\n",
    "\n",
    "for line in dialogure_lines:\n",
    "    print(line)\n",
    "\n",
    "print(f\"Summary of the Dialogure is: {dataset['test'][200]['summary']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b15f0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Input Prompt: \n",
      "\"\n",
      "Summarize the following conversation.\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "BaseLine Human Summary: \n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "Model Generated - Zero Shot :\n",
      "#Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\"\n",
    "Summarize the following conversation.\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(inputs[\"input_ids\"],\n",
    "    max_new_tokens = 200,\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "dash_line = '-'.join('' for x in range(80))\n",
    "print(dash_line)\n",
    "\n",
    "print(f'Input Prompt: \\n{prompt}')\n",
    "print(dash_line)\n",
    "\n",
    "print(f\"BaseLine Human Summary: \\n{summary}\\n\")\n",
    "print(dash_line)\n",
    "\n",
    "print(f\"Model Generated - Zero Shot :\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa157f",
   "metadata": {},
   "source": [
    "# Performance Full Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6f016f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = \"Summarize the following conversation. \\n\\n\"\n",
    "    end_prompt = \"\\n\\n Summary:\"\n",
    "\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, truncation = True, padding = \"max_length\", return_tensors='pt').input_ids\n",
    "    example['labels'] = tokenizer(example['summary'], truncation = True, padding='max_length', return_tensors='pt').input_ids\n",
    "\n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.ipynb\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id','topic','dialogue','summary',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ff5eab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 12460/12460 [00:03<00:00, 3190.95 examples/s]\n",
      "Filter: 100%|██████████| 500/500 [00:00<00:00, 3116.05 examples/s]\n",
      "Filter: 100%|██████████| 1500/1500 [00:00<00:00, 3229.77 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (125, 2)\n",
      "Validation: (5, 2)\n",
      "Test: (15, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 125\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 15\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now lets check the shapes of all three parts of the dataset but we will first take\n",
    "# a subset to the data\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index %100 ==0, with_indices=True)\n",
    "print(f\"Shapes of the datasets:\")\n",
    "\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b0fe2f",
   "metadata": {},
   "source": [
    "# Fine Tune the Model with PreProcessed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d855b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# Now we will utilize the builtin Hugging Face Trainer Class\n",
    "\n",
    "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1,\n",
    "    per_device_train_batch_size=2, # added for gpu but didnt workout\n",
    "    per_device_eval_batch_size=2, #added for gpu but didnt workout\n",
    "    fp16=True, #added for gpu but didnt workout\n",
    ")\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "trainer = Trainer (\n",
    "    model = original_model,\n",
    "    args = training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset = tokenized_datasets['validation']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9349a562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:01, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>46.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=46.5, metrics={'train_runtime': 521.9627, 'train_samples_per_second': 0.004, 'train_steps_per_second': 0.002, 'total_flos': 1369514704896.0, 'train_loss': 46.5, 'epoch': 0.015873015873015872})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "104b21c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trained_model_dir = \"./trained_model\"\n",
    "trainer.save_model(trained_model_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "583ed21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the saved model\n",
    "trained_model = AutoModelForSeq2SeqLM.from_pretrained(trained_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91c7d3d",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de18faf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Baseline Human Summary: \n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "--------------------------------------------------\n",
      "Original Model : \n",
      " #Person1#: I'm thinking of upgrading your computer hardware. #Person2#: I'm not sure what I'm looking at. #Person1#: I'm not sure what I need. #Person2#: I'm not sure what I need. #Person1#: I'm not sure what I need. #Person2#: I'm not sure what I need. #Person1#: I'm not sure what I need. #Person2#: I'm not sure what I need. #Person1#: I'm not sure what I need. #Person2#: I'm not sure what I need. #Person1#: I'm not sure what I need. #Person2#: I'm not sure what I need. #Person1#: I'm not sure.\n",
      "--------------------------------------------------\n",
      "Trained Model: \n",
      " #Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the prompt\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "\n",
    "#Ensure that input_ids are on the same device and the model.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_ids = input_ids.to(device)\n",
    "original_model.to(device)\n",
    "trained_model.to(device)\n",
    "\n",
    "\n",
    "#Generate outputs using the original model before training\n",
    "generation_config = GenerationConfig(max_new_tokens=200,num_beams=1)\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=generation_config)\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0],skip_special_tokens=True)\n",
    "\n",
    "#Generate outputs using the trained model after training\n",
    "trained_model_outputs = trained_model.generate(input_ids = input_ids, generation_config=generation_config)\n",
    "trained_model_text_output = tokenizer.decode(trained_model_outputs[0],skip_special_tokens=True)\n",
    "\n",
    "\n",
    "human_baseline_summary = summary\n",
    "dash_line = '-' *50\n",
    "\n",
    "print(dash_line)\n",
    "print(f\"Baseline Human Summary: \\n{human_baseline_summary}\")\n",
    "print(dash_line)\n",
    "print(f\"Original Model : \\n {original_model_text_output}\")\n",
    "print(dash_line)\n",
    "print(f\"Trained Model: \\n {trained_model_text_output}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302d8fec",
   "metadata": {},
   "source": [
    "# Evaluate the Model Quantitatively with ROUGE Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0be617b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 6.27kB [00:00, 545kB/s]\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f305d515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Human_baseline_Summaries",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Original_model_summaries",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "instruct_model_summaries",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "20d9f655-8aca-469a-a65b-18cf2251d267",
       "rows": [
        [
         "0",
         "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.",
         "\"",
         "The following memo is to be distributed to all employees by this afternoon."
        ],
        [
         "1",
         "In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.",
         "Employees are required to sign a dictation before 4 pm.",
         "The following memo is to be distributed to all employees by this afternoon."
        ],
        [
         "2",
         "Ms. Dawson takes a dictation for #Person1# about prohibiting the use of Instant Message programs in the office. They argue about its reasonability but #Person1# still insists.",
         "Employees are required to take a dictation from a manager.",
         "The following memo is to be distributed to all employees by this afternoon."
        ],
        [
         "3",
         "#Person2# arrives late because of traffic jam. #Person1# persuades #Person2# to use public transportations to keep healthy and to protect the environment.",
         "The conversation is about the traffic james in the city.",
         "The traffic jam at the Carrefour intersection is a problem for Person1 and Person2 who are trying to get to work."
        ],
        [
         "4",
         "#Person2# decides to follow #Person1#'s suggestions on quitting driving to work and will try to use public transportations.",
         "The person who is driving to work is upset about the traffic.",
         "The traffic jam at the Carrefour intersection is a problem for Person1 and Person2 who are trying to get to work."
        ],
        [
         "5",
         "#Person2# complains to #Person1# about the traffic jam, #Person1# suggests quitting driving and taking public transportation instead.",
         "#Person1: You're finally here. What took so long? #Person2: It took so long. #Person1: It's a long time. #Person2: I'm sorry. #Person1: I'm not sure. #Person2: I'm not sure. #Person1: I'm not sure. #Person2: I'm not sure. #Person1: I'm not sure. #Person2: I'm not sure. #Person1: I'm not sure. #Person2: I'm sorry. #Person1: I'm sorry. #Person2: I'm sorry. #Person2: I'm sorry. #Person1: I'm sorry. #Person2: I'm sorry. #Person1: I'm sorry. #Person2",
         "The traffic jam at the Carrefour intersection is a problem for Person1 and Person2 who are trying to get to work."
        ],
        [
         "6",
         "#Person1# tells Kate that Masha and Hero get divorced. Kate is surprised because she thought they are perfect couple.",
         "#Person1: Masha and Hero are getting divorced.",
         "Masha and Hero are getting divorced."
        ],
        [
         "7",
         "#Person1# tells Kate that Masha and Hero are getting a peaceful divorce. Kate feels surprised and asks about their kids.",
         "Masha and Hero are getting divorced.",
         "Masha and Hero are getting divorced."
        ],
        [
         "8",
         "#Person1# and Kate talk about the divorce between Masha and Hero. Kate feels surprised because she thought they are well matched",
         "Masha and Hero are getting divorced.",
         "Masha and Hero are getting divorced."
        ],
        [
         "9",
         "#Person1# and Brian are at the birthday party of Brian. Brian thinks #Person1# looks great and is popular.",
         "#Person1: Thank you, Brian. #Person2: I hope you look pretty today. #Person1: Thank you, I hope you look good. #Person1: I hope you look good. #Person2: Thank you, Brian. #Person1: Thank you, I hope you have a good day. #Person2: Thank you, I hope you have a good day. #Person2: Thank you, I hope you have a good day. #Person1: Thank you, I hope you have a good day.",
         "#Person1: Happy Birthday, Brian. #Person2: Thank you for coming. #Person1: Thank you for coming. #Person2: Thank you for coming. #Person1: Thank you for coming. #Person2: Thank you for coming. #Person1: Thank you for coming. #Person2: Thank you for coming. #Person1: Thank you for coming. #Person2: Thank you for coming. #Person1: Thank you for coming. #Person2: Thank you for coming. #Person1: Thank you for coming. #Person2: Thank you for coming. #Person1: Thank you for coming. #Person2: Thank you for coming. #Person1: Thank you for coming. #Person2: Thank you for coming. #Person1: Thank you for coming. #Person2: Thank you for coming."
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Human_baseline_Summaries</th>\n",
       "      <th>Original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>\"</td>\n",
       "      <td>The following memo is to be distributed to all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>Employees are required to sign a dictation bef...</td>\n",
       "      <td>The following memo is to be distributed to all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>Employees are required to take a dictation fro...</td>\n",
       "      <td>The following memo is to be distributed to all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>The conversation is about the traffic james in...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>The person who is driving to work is upset abo...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>#Person1: You're finally here. What took so lo...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>#Person1: Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>#Person1: Thank you, Brian. #Person2: I hope y...</td>\n",
       "      <td>#Person1: Happy Birthday, Brian. #Person2: Tha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Human_baseline_Summaries  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "5  #Person2# complains to #Person1# about the tra...   \n",
       "6  #Person1# tells Kate that Masha and Hero get d...   \n",
       "7  #Person1# tells Kate that Masha and Hero are g...   \n",
       "8  #Person1# and Kate talk about the divorce betw...   \n",
       "9  #Person1# and Brian are at the birthday party ...   \n",
       "\n",
       "                            Original_model_summaries  \\\n",
       "0                                                  \"   \n",
       "1  Employees are required to sign a dictation bef...   \n",
       "2  Employees are required to take a dictation fro...   \n",
       "3  The conversation is about the traffic james in...   \n",
       "4  The person who is driving to work is upset abo...   \n",
       "5  #Person1: You're finally here. What took so lo...   \n",
       "6     #Person1: Masha and Hero are getting divorced.   \n",
       "7               Masha and Hero are getting divorced.   \n",
       "8               Masha and Hero are getting divorced.   \n",
       "9  #Person1: Thank you, Brian. #Person2: I hope y...   \n",
       "\n",
       "                            instruct_model_summaries  \n",
       "0  The following memo is to be distributed to all...  \n",
       "1  The following memo is to be distributed to all...  \n",
       "2  The following memo is to be distributed to all...  \n",
       "3  The traffic jam at the Carrefour intersection ...  \n",
       "4  The traffic jam at the Carrefour intersection ...  \n",
       "5  The traffic jam at the Carrefour intersection ...  \n",
       "6               Masha and Hero are getting divorced.  \n",
       "7               Masha and Hero are getting divorced.  \n",
       "8               Masha and Hero are getting divorced.  \n",
       "9  #Person1: Happy Birthday, Brian. #Person2: Tha...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogue = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "\n",
    "for _, dialogure in enumerate(dialogue):\n",
    "    prompt = f\"\"\"\"Summarize the following conversation\n",
    "    {dialogure}\n",
    "    Summary:\n",
    "\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "    #Ensure the the input_ids and mode are on the same device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids,generation_config = GenerationConfig(max_new_tokens=200,num_beams=1))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0],skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "\n",
    "    instruct_model_outputs = trained_model.generate(input_ids = input_ids,generation_config = GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0],skip_special_tokens=True)\n",
    "    instruct_model_summaries.append(instruct_model_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries,original_model_summaries,instruct_model_summaries))\n",
    "df = pd.DataFrame(zipped_summaries,columns=['Human_baseline_Summaries','Original_model_summaries','instruct_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "422e85b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model:\n",
      "[{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}, {'rouge-1': {'r': 0.06451612903225806, 'p': 0.2, 'f': 0.09756097192147545}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.06451612903225806, 'p': 0.2, 'f': 0.09756097192147545}}, {'rouge-1': {'r': 0.08333333333333333, 'p': 0.2222222222222222, 'f': 0.1212121172451792}, 'rouge-2': {'r': 0.038461538461538464, 'p': 0.1111111111111111, 'f': 0.05714285332244924}, 'rouge-l': {'r': 0.08333333333333333, 'p': 0.2222222222222222, 'f': 0.1212121172451792}}, {'rouge-1': {'r': 0.10526315789473684, 'p': 0.2222222222222222, 'f': 0.1428571384948981}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.10526315789473684, 'p': 0.2222222222222222, 'f': 0.1428571384948981}}, {'rouge-1': {'r': 0.1875, 'p': 0.2727272727272727, 'f': 0.22222221739369008}, 'rouge-2': {'r': 0.11764705882352941, 'p': 0.18181818181818182, 'f': 0.14285713808673486}, 'rouge-l': {'r': 0.1875, 'p': 0.2727272727272727, 'f': 0.22222221739369008}}, {'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}, {'rouge-1': {'r': 0.2777777777777778, 'p': 0.7142857142857143, 'f': 0.39999999596800007}, 'rouge-2': {'r': 0.1111111111111111, 'p': 0.3333333333333333, 'f': 0.16666666291666676}, 'rouge-l': {'r': 0.2777777777777778, 'p': 0.7142857142857143, 'f': 0.39999999596800007}}, {'rouge-1': {'r': 0.2777777777777778, 'p': 0.8333333333333334, 'f': 0.41666666291666676}, 'rouge-2': {'r': 0.21052631578947367, 'p': 0.8, 'f': 0.33333333003472226}, 'rouge-l': {'r': 0.2777777777777778, 'p': 0.8333333333333334, 'f': 0.41666666291666676}}, {'rouge-1': {'r': 0.21052631578947367, 'p': 0.6666666666666666, 'f': 0.31999999635200005}, 'rouge-2': {'r': 0.1, 'p': 0.4, 'f': 0.15999999680000007}, 'rouge-l': {'r': 0.21052631578947367, 'p': 0.6666666666666666, 'f': 0.31999999635200005}}, {'rouge-1': {'r': 0.07142857142857142, 'p': 0.06666666666666667, 'f': 0.06896551224732497}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.06666666666666667, 'f': 0.06896551224732497}}]\n",
      "Instruct Model:\n",
      "[{'rouge-1': {'r': 0.08, 'p': 0.16666666666666666, 'f': 0.10810810372534715}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.08, 'p': 0.16666666666666666, 'f': 0.10810810372534715}}, {'rouge-1': {'r': 0.1935483870967742, 'p': 0.5, 'f': 0.27906976341806383}, 'rouge-2': {'r': 0.08571428571428572, 'p': 0.25, 'f': 0.12765957066545958}, 'rouge-l': {'r': 0.1935483870967742, 'p': 0.5, 'f': 0.27906976341806383}}, {'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}, {'rouge-1': {'r': 0.2631578947368421, 'p': 0.25, 'f': 0.2564102514135438}, 'rouge-2': {'r': 0.047619047619047616, 'p': 0.05, 'f': 0.048780482807852986}, 'rouge-l': {'r': 0.21052631578947367, 'p': 0.2, 'f': 0.20512820013149255}}, {'rouge-1': {'r': 0.1875, 'p': 0.15, 'f': 0.16666666172839517}, 'rouge-2': {'r': 0.058823529411764705, 'p': 0.05, 'f': 0.05405404908692522}, 'rouge-l': {'r': 0.125, 'p': 0.1, 'f': 0.11111110617283973}}, {'rouge-1': {'r': 0.25, 'p': 0.2, 'f': 0.22222221728395072}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.125, 'p': 0.1, 'f': 0.11111110617283973}}, {'rouge-1': {'r': 0.2777777777777778, 'p': 0.8333333333333334, 'f': 0.41666666291666676}, 'rouge-2': {'r': 0.1111111111111111, 'p': 0.4, 'f': 0.17391304007561445}, 'rouge-l': {'r': 0.2777777777777778, 'p': 0.8333333333333334, 'f': 0.41666666291666676}}, {'rouge-1': {'r': 0.2777777777777778, 'p': 0.8333333333333334, 'f': 0.41666666291666676}, 'rouge-2': {'r': 0.21052631578947367, 'p': 0.8, 'f': 0.33333333003472226}, 'rouge-l': {'r': 0.2777777777777778, 'p': 0.8333333333333334, 'f': 0.41666666291666676}}, {'rouge-1': {'r': 0.21052631578947367, 'p': 0.6666666666666666, 'f': 0.31999999635200005}, 'rouge-2': {'r': 0.1, 'p': 0.4, 'f': 0.15999999680000007}, 'rouge-l': {'r': 0.21052631578947367, 'p': 0.6666666666666666, 'f': 0.31999999635200005}}, {'rouge-1': {'r': 0.07142857142857142, 'p': 0.1111111111111111, 'f': 0.0869565169754256}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.1111111111111111, 'f': 0.0869565169754256}}]\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "\n",
    "\n",
    "original_model_results = rouge.get_scores(\n",
    "    original_model_summaries,\n",
    "    human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    )\n",
    "\n",
    "instruct_model_results = rouge.get_scores(\n",
    "    instruct_model_summaries,\n",
    "    human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    )\n",
    "\n",
    "print('Original Model:')\n",
    "print(original_model_results)\n",
    "\n",
    "print('Instruct Model:')\n",
    "print(instruct_model_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
