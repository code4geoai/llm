{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28d55a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\llmenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afc0954a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6688c0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'train_0',\n",
       " 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n",
       " 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n",
       " 'topic': 'get a check-up'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "002d1b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\n",
      "#Person2#: I found it would be a good idea to get a check-up.\n",
      "#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\n",
      "#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\n",
      "#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\n",
      "#Person2#: Ok.\n",
      "#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\n",
      "#Person2#: Yes.\n",
      "#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\n",
      "#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\n",
      "#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\n",
      "#Person2#: Ok, thanks doctor.\n"
     ]
    }
   ],
   "source": [
    "example = dataset['train'][0]\n",
    "dialogure_lines = example['dialogue'].split('\\n')\n",
    "\n",
    "for line in dialogure_lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43f6a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Model\n",
    "model_name = 'google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name , torch_dtype = torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "785e02d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8774,  296,    1]])\n",
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "#Example :\n",
    "\n",
    "prompt = \"Hello world\"\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\") #pt is for pytorch\n",
    "\n",
    "print(tokens[\"input_ids\"])\n",
    "print(tokenizer.decode(tokens[\"input_ids\"][0],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eb3ce5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856\n",
      " All model parameters:247577856 \n",
      " Percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# To check the trainable parameters\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\n All model parameters:{all_model_params} \\n Percentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40173379",
   "metadata": {},
   "source": [
    "# Test the Model with Zero Shot Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15a52e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "Summary of the Dialogure is: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n"
     ]
    }
   ],
   "source": [
    "# Lets utilize this dialogue for inference\n",
    "example = dataset['test'][200]\n",
    "dialogure_lines = example['dialogue'].split('\\n')\n",
    "\n",
    "for line in dialogure_lines:\n",
    "    print(line)\n",
    "\n",
    "print(f\"Summary of the Dialogure is: {dataset['test'][200]['summary']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b15f0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Input Prompt: \n",
      "\"\n",
      "Summarize the following conversation.\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "BaseLine Human Summary: \n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "Model Generated - Zero Shot :\n",
      "#Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\"\n",
    "Summarize the following conversation.\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(inputs[\"input_ids\"],\n",
    "    max_new_tokens = 200,\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "dash_line = '-'.join('' for x in range(80))\n",
    "print(dash_line)\n",
    "\n",
    "print(f'Input Prompt: \\n{prompt}')\n",
    "print(dash_line)\n",
    "\n",
    "print(f\"BaseLine Human Summary: \\n{summary}\\n\")\n",
    "print(dash_line)\n",
    "\n",
    "print(f\"Model Generated - Zero Shot :\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa157f",
   "metadata": {},
   "source": [
    "# Performance Full Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6f016f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = \"Summarize the following conversation. \\n\\n\"\n",
    "    end_prompt = \"\\n\\n Summary:\"\n",
    "\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, truncation = True, padding = \"max_length\", return_tensors='pt').input_ids\n",
    "    example['labels'] = tokenizer(example['summary'], truncation = True, padding='max_length', return_tensors='pt').input_ids\n",
    "\n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.ipynb\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id','topic','dialogue','summary',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ff5eab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 12460/12460 [00:04<00:00, 3113.71 examples/s]\n",
      "Filter: 100%|██████████| 500/500 [00:00<00:00, 2857.63 examples/s]\n",
      "Filter: 100%|██████████| 1500/1500 [00:00<00:00, 3110.87 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (125, 2)\n",
      "Validation: (5, 2)\n",
      "Test: (15, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 125\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 15\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now lets check the shapes of all three parts of the dataset but we will first take\n",
    "# a subset to the data\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index %100 ==0, with_indices=True)\n",
    "print(f\"Shapes of the datasets:\")\n",
    "\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b0fe2f",
   "metadata": {},
   "source": [
    "# Fine Tune the Model with PreProcessed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9d855b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# Now we will utilize the builtin Hugging Face Trainer Class\n",
    "\n",
    "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer (\n",
    "    model = original_model,\n",
    "    args = training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset = tokenized_datasets['validation']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9349a562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:01, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>47.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=47.75, metrics={'train_runtime': 2027.8251, 'train_samples_per_second': 0.004, 'train_steps_per_second': 0.0, 'total_flos': 5478058819584.0, 'train_loss': 47.75, 'epoch': 0.0625})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "104b21c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trained_model_dir = \"./trained_model\"\n",
    "trainer.save_model(trained_model_dir)\n",
    "\n",
    "#load the saved model\n",
    "trained_model = AutoModelForSeq2SeqLM.from_pretrained(trained_model_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
