{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "459cc703",
   "metadata": {},
   "source": [
    "# Parameter Efficient Fine Tuning(PEFT)   \n",
    "This technique uses LORA i.e. Lower Rank Adaptation and prompt tuning. After fine tuning with LORA, the original LLM remains unchanged and new LORA Adapter emerges. This LORA adapter is much smaller in size and use fewer resources as compare to original LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36e57175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6213d23f",
   "metadata": {
    "vscode": {
     "languageId": "perl"
    }
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules = [\"q\", \"v\"],\n",
    "    lora_dropout = 0.05,\n",
    "    bias = 'none',\n",
    "    task_type= TaskType.SEQ_2_SEQ_LM #FLAN-T5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f82021a",
   "metadata": {
    "vscode": {
     "languageId": "perl"
    }
   },
   "outputs": [],
   "source": [
    "# Load the Model\n",
    "model_name = 'google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name , torch_dtype = torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b98ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 3538944\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 1.41%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e27980b",
   "metadata": {
    "vscode": {
     "languageId": "perl"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model:\n",
      "trainable model parameters: 3538944\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 1.41%\n",
      "--------------------------------------------------\n",
      "PEFT Model:\n",
      "trainable model parameters: 3538944\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 1.41%\n"
     ]
    }
   ],
   "source": [
    "# Now lets Add LoRA to the Original Model\n",
    "\n",
    "peft_model = get_peft_model(original_model,lora_config)\n",
    "\n",
    "print(\"Original Model:\")\n",
    "print(print_number_of_trainable_model_parameters(original_model))\n",
    "print(\"-\" * 50)\n",
    "print(\"PEFT Model:\")\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a119c885",
   "metadata": {},
   "source": [
    "# Train PEFT Adapter\n",
    "\n",
    "Lets first define the dataset and then tokenize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e8c028d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets Load the dataset\n",
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16f098bc",
   "metadata": {
    "vscode": {
     "languageId": "perl"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = \"Summarize the following conversation. \\n\\n\"\n",
    "    end_prompt = \"\\n\\n Summary:\"\n",
    "\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, truncation = True, padding = \"max_length\", return_tensors='pt').input_ids\n",
    "    example['labels'] = tokenizer(example['summary'], truncation = True, padding='max_length', return_tensors='pt').input_ids\n",
    "\n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.ipynb\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id','topic','dialogue','summary',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0b8a0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "output_dir = f\"./dialogue-summary-training-{str(int(time.time()))}\"\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir= output_dir,\n",
    "    auto_find_batch_size = True,\n",
    "    learning_rate = 1e-3,\n",
    "    num_train_epochs = 1,\n",
    "    logging_steps = 1,\n",
    "    max_steps = 1\n",
    ")\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model = peft_model,\n",
    "    args = peft_training_args,\n",
    "    train_dataset = tokenized_datasets['train']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52b91a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>47.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./peft-dialogue-summary-checkpoint-local\\\\tokenizer_config.json',\n",
       " './peft-dialogue-summary-checkpoint-local\\\\special_tokens_map.json',\n",
       " './peft-dialogue-summary-checkpoint-local\\\\tokenizer.json')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now everything is ready to train the PEFT Adapter model and save it\n",
    "\n",
    "peft_trainer.train()\n",
    "\n",
    "peft_model_path = \"./peft-dialogue-summary-checkpoint-local\"\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0c31a1d",
   "metadata": {
    "vscode": {
     "languageId": "perl"
    }
   },
   "outputs": [],
   "source": [
    "# Finally we will load hte save Fine tuned model\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\",torch_dtype = torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base,\n",
    "                                        peft_model_path,\n",
    "                                        torch_dtype=torch.bfloat16, \n",
    "                                        is_trainable=False\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77047e05",
   "metadata": {},
   "source": [
    "# Evaluate the Model Qualitatively   \n",
    "Now lets test the PEFT fine tuned model and see whether it is able to create a reasonable summary of the dialogure compared to the original and full fined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1b70b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the saved model\n",
    "trained_model_dir = \"./trained_model\"\n",
    "trained_model = AutoModelForSeq2SeqLM.from_pretrained(trained_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679b80fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1#: Have you considered upgrading your system? #Person2#: Yes, but I'm not sure what to do about it. #Person1#: You could consider adding a painting program to your software. #Person2#: I'd probably need a faster processor, more memory, and a faster modem. #Person1#: You could also consider adding a CD-ROM drive. #Person2#: I'd probably need a CD-ROM drive. #Person1#: That's great.\n",
      "--------------------------------------------------\n",
      "Human Summary: \n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "--------------------------------------------------\n",
      "Original Model Summary:\n",
      "#Person1#: Have you considered upgrading your system? #Person2#: Yes, but I'm not sure what to do about it. #Person1#: You could consider adding a painting program to your software. #Person2#: I'd probably need a faster processor, more memory, and a faster modem. #Person1#: You could also consider adding a CD-ROM drive. #Person2#: I'd probably need a CD-ROM drive. #Person1#: That's great.\n",
      "--------------------------------------------------\n",
      "Trained Model Output:\n",
      " #Person1#: I'm thinking of upgrading my computer.\n",
      "--------------------------------------------------\n",
      "PEFT Model Output:\n",
      " #Person1#: I'm thinking of upgrading my computer.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the Model with Zero Shot Inferencing\n",
    "index  = 200\n",
    "\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary= dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt,return_tensors = 'pt').input_ids\n",
    "\n",
    "#Ensure that the input_ids and the models are on the same device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_ids = input_ids.to(device)\n",
    "original_model.to(device)\n",
    "trained_model.to(device)\n",
    "peft_model.to(device)\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids = input_ids, generation_config= GenerationConfig(max_new_tokens=200,num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0],skip_special_tokens=True)\n",
    "print(original_model_text_output)\n",
    "\n",
    "instruct_model_outputs = trained_model.generate(input_ids= input_ids, generation_config = GenerationConfig(max_new_tokens = 200, num_beams=1))\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0],skip_special_tokens = True)\n",
    "\n",
    "peft_model_outputs = peft_model.generate(input_ids = input_ids, generation_config = GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0],skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(f\"Human Summary: \\n{summary}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Original Model Summary:\\n{original_model_text_output}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Instruct Model Output:\\n {instruct_model_text_output}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"PEFT Model Output:\\n {peft_model_text_output}\")\n",
    "print(\"-\"*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3162ed33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "human_baseline_summaries",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "original_model_summaries",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "instruct_model_summaries",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "peft_model_summaries",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "f2f3abe1-b881-462e-8eb3-56dec02862db",
       "rows": [
        [
         "0",
         "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.",
         "New intra-office memorandum goes out today.",
         "#Person1#: This is an intra-office memo. #Person2: This memo is for all employees. #Person1: This memo is for all employees. #Person2: This memo is for all employees. #Person1: This memo is for all employees.",
         "This memo is to be distributed to all employees by this afternoon."
        ],
        [
         "1",
         "In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.",
         "#Person1#: Ms. Dawson, I need you to take a dictation for me.",
         "The following are the following rules for communication in the office: Using instant messaging.",
         "This memo is to be distributed to all employees by this afternoon."
        ],
        [
         "2",
         "Ms. Dawson takes a dictation for #Person1# about prohibiting the use of Instant Message programs in the office. They argue about its reasonability but #Person1# still insists.",
         "Memo to all employees.",
         "This memo is to be distributed to all employees by this afternoon.",
         "This memo is to be distributed to all employees by this afternoon."
        ],
        [
         "3",
         "#Person2# arrives late because of traffic jam. #Person1# persuades #Person2# to use public transportations to keep healthy and to protect the environment.",
         "#Person1#: You're finally here! #Person2#: You're right! #Person1#: You're right! #Person2#: Perhaps it would be better if you started taking public transport to work. #Person1#: Perhaps it would be better for you to take the subway. #Person2#: Taking the subway would be a lot less stressful than driving. #Person1#: Taking the subway would be a lot less stressful than driving. #Person2#: I'm going to miss having the freedom that you have with a car. #Person1#: Taking a bike to work. #Person2#: Taking the subway would be a lot less stressful than driving. #Person1#: Taking the subway would be a lot less stressful than driving. #Person1#: Taking the subway",
         "#Person1: I'm so excited for you. I'm so glad to hear that you're finally here.",
         "The traffic jam at the Carrefour intersection is a problem for Person1 and Person2 who are both driving to work."
        ],
        [
         "4",
         "#Person2# decides to follow #Person1#'s suggestions on quitting driving to work and will try to use public transportations.",
         "The driver is stuck in traffic.",
         "The car is a waste of time and money.",
         "The traffic jam at the Carrefour intersection is a problem for Person1 and Person2 who are both driving to work."
        ],
        [
         "5",
         "#Person2# complains to #Person1# about the traffic jam, #Person1# suggests quitting driving and taking public transportation instead.",
         "#Person1#: I'm not sure what happened to me.",
         "The driver's car is a problem for the driver.",
         "The traffic jam at the Carrefour intersection is a problem for Person1 and Person2 who are both driving to work."
        ],
        [
         "6",
         "#Person1# tells Kate that Masha and Hero get divorced. Kate is surprised because she thought they are perfect couple.",
         "Masha and Hero are divorced.",
         "#Person1: Masha and Hero are getting divorced. #Person2: I don't really know. #Person1: Well, I heard that they are having a separation for 2 months, and filed for divorce. #Person2: Well, I still can't believe it. Masha and Hero are getting divorced.",
         "Masha and Hero are getting divorced."
        ],
        [
         "7",
         "#Person1# tells Kate that Masha and Hero are getting a peaceful divorce. Kate feels surprised and asks about their kids.",
         "Masha and Hero are having a separation for 2 months.",
         "Masha and Hero are getting divorced.",
         "Masha and Hero are getting divorced."
        ],
        [
         "8",
         "#Person1# and Kate talk about the divorce between Masha and Hero. Kate feels surprised because she thought they are well matched",
         "#Person1: Masha and Hero are getting divorced.",
         "#Person1: Kate, you never believe what's happened.",
         "Masha and Hero are getting divorced."
        ],
        [
         "9",
         "#Person1# and Brian are at the birthday party of Brian. Brian thinks #Person1# looks great and is popular.",
         "Brian is a guest at the party.",
         "Happy Birthday, Brian.",
         "Brian's birthday is coming up."
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "      <th>peft_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>New intra-office memorandum goes out today.</td>\n",
       "      <td>#Person1#: This is an intra-office memo. #Pers...</td>\n",
       "      <td>This memo is to be distributed to all employee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>#Person1#: Ms. Dawson, I need you to take a di...</td>\n",
       "      <td>The following are the following rules for comm...</td>\n",
       "      <td>This memo is to be distributed to all employee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>Memo to all employees.</td>\n",
       "      <td>This memo is to be distributed to all employee...</td>\n",
       "      <td>This memo is to be distributed to all employee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>#Person1#: You're finally here! #Person2#: You...</td>\n",
       "      <td>#Person1: I'm so excited for you. I'm so glad ...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>The driver is stuck in traffic.</td>\n",
       "      <td>The car is a waste of time and money.</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>#Person1#: I'm not sure what happened to me.</td>\n",
       "      <td>The driver's car is a problem for the driver.</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>Masha and Hero are divorced.</td>\n",
       "      <td>#Person1: Masha and Hero are getting divorced....</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>Masha and Hero are having a separation for 2 m...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>#Person1: Masha and Hero are getting divorced.</td>\n",
       "      <td>#Person1: Kate, you never believe what's happe...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>Brian is a guest at the party.</td>\n",
       "      <td>Happy Birthday, Brian.</td>\n",
       "      <td>Brian's birthday is coming up.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "5  #Person2# complains to #Person1# about the tra...   \n",
       "6  #Person1# tells Kate that Masha and Hero get d...   \n",
       "7  #Person1# tells Kate that Masha and Hero are g...   \n",
       "8  #Person1# and Kate talk about the divorce betw...   \n",
       "9  #Person1# and Brian are at the birthday party ...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0        New intra-office memorandum goes out today.   \n",
       "1  #Person1#: Ms. Dawson, I need you to take a di...   \n",
       "2                             Memo to all employees.   \n",
       "3  #Person1#: You're finally here! #Person2#: You...   \n",
       "4                    The driver is stuck in traffic.   \n",
       "5       #Person1#: I'm not sure what happened to me.   \n",
       "6                       Masha and Hero are divorced.   \n",
       "7  Masha and Hero are having a separation for 2 m...   \n",
       "8     #Person1: Masha and Hero are getting divorced.   \n",
       "9                     Brian is a guest at the party.   \n",
       "\n",
       "                            instruct_model_summaries  \\\n",
       "0  #Person1#: This is an intra-office memo. #Pers...   \n",
       "1  The following are the following rules for comm...   \n",
       "2  This memo is to be distributed to all employee...   \n",
       "3  #Person1: I'm so excited for you. I'm so glad ...   \n",
       "4              The car is a waste of time and money.   \n",
       "5      The driver's car is a problem for the driver.   \n",
       "6  #Person1: Masha and Hero are getting divorced....   \n",
       "7               Masha and Hero are getting divorced.   \n",
       "8  #Person1: Kate, you never believe what's happe...   \n",
       "9                             Happy Birthday, Brian.   \n",
       "\n",
       "                                peft_model_summaries  \n",
       "0  This memo is to be distributed to all employee...  \n",
       "1  This memo is to be distributed to all employee...  \n",
       "2  This memo is to be distributed to all employee...  \n",
       "3  The traffic jam at the Carrefour intersection ...  \n",
       "4  The traffic jam at the Carrefour intersection ...  \n",
       "5  The traffic jam at the Carrefour intersection ...  \n",
       "6               Masha and Hero are getting divorced.  \n",
       "7               Masha and Hero are getting divorced.  \n",
       "8               Masha and Hero are getting divorced.  \n",
       "9                     Brian's birthday is coming up.  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate the Model Quantitatively with ROUGE Metric\n",
    "\n",
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for idx, dialogue in enumerate(dialogues):\n",
    "  prompt = f\"\"\"\n",
    "  summarize the following conversation\n",
    "  {dialogue}\n",
    "  Summary:\n",
    "\n",
    "  \"\"\"\n",
    "  input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "  # Ensure that input_ids and the models are on the same device\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  input_ids = input_ids.to(device)\n",
    "\n",
    "  human_baseline_text_output = human_baseline_summaries[idx]\n",
    "\n",
    "  original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "  original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "  original_model_summaries.append(original_model_text_output)\n",
    "\n",
    "  instruct_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "  instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "  instruct_model_summaries.append(instruct_model_text_output)\n",
    "\n",
    "  peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "  peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "  peft_model_summaries.append(peft_model_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns=['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries'])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
